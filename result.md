# ML Pipeline Results

## 01_Data_Loading_and_Mapping

ğŸš€ **Starting Complete ML Pipeline**

ğŸ“Š **Loading and mapping data**

- Data shape: (10000, 8)
- Using `Failure_Type_Encoded` as target

**Original target distribution:**

| Failure Type             | Count | %     |
| ------------------------ | ----- | ----- |
| No Failure               | 9652  | 96.5% |
| Heat Dissipation Failure | 112   | 1.1%  |
| Overstrain Failure       | 78    | 0.8%  |
| Power Failure            | 95    | 0.9%  |
| Random Failure           | 18    | 0.2%  |
| Tool Wear Failure        | 45    | 0.4%  |

---

## 02_Scaling_and_Preprocessing

âš–ï¸ **Scaling numerical features before split**

- Features scaled: `['Air_temperature_K', 'Process_temperature_K', 'Rotational_speed_rpm', 'Torque_Nm', 'Tool_wear_min']`
- Scaling saved as `scaled.pkl`

---

## 03_Train_Test_Split

ğŸ”„ **Splitting data**

- Training set shape: (8000, 6)
- Test set shape: (2000, 6)

---

## 04_Before_Balancing_Distribution

ğŸ“Š **Training class distribution BEFORE balancing:**

| Failure Type             | Count | %      |
| ------------------------ | ----- | ------ |
| No Failure               | 7,722 | 96.53% |
| Heat Dissipation Failure | 90    | 1.12%  |
| Overstrain Failure       | 62    | 0.78%  |
| Power Failure            | 76    | 0.95%  |
| Random Failure           | 14    | 0.18%  |
| Tool Wear Failure        | 36    | 0.45%  |

---

## 05_SMOTE_Balancing

ğŸ§¬ **Applying SMOTE to balance failure types**

- Target samples per class: 2,500

**SMOTE sampling:**

| Failure Type             | Before | After |
| ------------------------ | ------ | ----- |
| Heat Dissipation Failure | 90     | 2,500 |
| Overstrain Failure       | 62     | 2,500 |
| Power Failure            | 76     | 2,500 |
| Random Failure           | 14     | 2,500 |
| Tool Wear Failure        | 36     | 2,500 |

---

## 06_After_Balancing_Distribution

ğŸ“Š **Training class distribution AFTER SMOTE balancing:**

| Failure Type             | Count | %      |
| ------------------------ | ----- | ------ |
| No Failure               | 7,722 | 38.19% |
| Heat Dissipation Failure | 2,500 | 12.36% |
| Overstrain Failure       | 2,500 | 12.36% |
| Power Failure            | 2,500 | 12.36% |
| Random Failure           | 2,500 | 12.36% |
| Tool Wear Failure        | 2,500 | 12.36% |

---

## 07_Class_Weights

ğŸ’ª **Updated class weights after balancing:**

| Failure Type             | Weight | Count |
| ------------------------ | ------ | ----- |
| Heat Dissipation Failure | 1.348  | 2,500 |
| Overstrain Failure       | 1.348  | 2,500 |
| Power Failure            | 1.348  | 2,500 |
| Random Failure           | 1.348  | 2,500 |
| Tool Wear Failure        | 1.348  | 2,500 |
| No Failure               | 0.436  | 7,722 |

---

## 08_Final_Data_Summary

âœ… **Final Balanced Training Data Summary**

- Original training samples: 8,000
- Balanced training samples: 20,222
- Test samples: 2,000
- Number of classes: 6
- No Failure samples preserved: 7,722
- Failure types balanced to ~2,500 each
- New imbalance ratio: 3.1:1
- Saved as `balanced_training_data.pkl`

---

## 09_Model_Training_and_Evaluation

ğŸ”¬ **Evaluating all models with class weighting**

**Summary of F1 scores (Test set):**

| Model              | Test F1 |
| ------------------ | ------- |
| RandomForest       | 0.9664  |
| GradientBoosting   | 0.9538  |
| ExtraTrees         | 0.9677  |
| SVM                | 0.8367  |
| LogisticRegression | 0.6287  |
| KNN                | 0.9257  |
| NaiveBayes         | 0.8269  |
| DecisionTree       | 0.9654  |
| MLP                | 0.9408  |
| AdaBoost           | 0.8930  |
| CatBoost           | 0.9601  |
| XGBoost            | 0.9707  |

---

## 10_Top_Models

ğŸ† **Top 5 Models:**

1. XGBoost: 0.9707
2. ExtraTrees: 0.9677
3. RandomForest: 0.9664
4. DecisionTree: 0.9654
5. CatBoost: 0.9601

---

## 11_Hyperparameter_Tuning

ğŸ¯ **Hyperparameter tuning for top 5 models**

**XGBoost**

- Best CV F1: 0.9918
- Test F1: 0.9729
- Best params: `{'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 300, 'subsample': 0.8}`

**ExtraTrees**

- Best CV F1: 0.9804
- Test F1: 0.9571
- Best params: `{'max_depth': 20, 'min_samples_split': 5, 'n_estimators': 300}`

**RandomForest**

- Best CV F1: 0.9779
- Test F1: 0.9590
- Best params: `{'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100}`

**DecisionTree**

- Best CV F1: 0.9629
- Test F1: 0.9487
- Best params: `{'max_depth': 20, 'min_samples_split': 5}`

**CatBoost**

- Best CV F1: 0.9776
- Test F1: 0.9627
- Best params: `{'depth': 8, 'iterations': 300, 'l2_leaf_reg': 1, 'learning_rate': 0.2}`

---

## 12_Final_Model_Saved

ğŸ’¾ **Saving best model: XGBoost**

- `best_model_final.pkl`
- `scaler_final.pkl`
- `pipeline_metadata.pkl`

**Class weights applied:**

- No Failure: 0.436
- Heat Dissipation Failure: 1.348
- Overstrain Failure: 1.348
- Power Failure: 1.348
- Random Failure: 1.348
- Tool Wear Failure: 1.348

---

## 12 âœ… Key Results from Analysis

- **Model Performance:**
  â€¢ Overall Accuracy: 97.15%
  â€¢ Weighted F1-Score: 97.61%
  â€¢ Tool Wear Detection: 98.59% F1-score (Excellent!)

- **Class-wise Performance:**
  â€¢ No Failure: 95.24% F1-score
  â€¢ Tool Wear: 98.59% F1-score (Best performing)
  â€¢ Overstrain: 90.00% F1-score
  â€¢ Heat Dissipation: 66.67% F1-score
  â€¢ Random Failure: 6.25% F1-score (Challenging class)
  â€¢ Power Failure: 0% F1-score (Very rare in test set)

---

## 13_Overall_Summary

ğŸ‰ **Pipeline Complete!**

- Best Model: XGBoost
- F1 Score: 0.9729
